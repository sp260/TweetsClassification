{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import pickle\n",
    "import codecs\n",
    "import spacy\n",
    "import glob\n",
    "import re\n",
    "\n",
    "all_files = glob.glob(\"./corpus/groupe*.txt\")\n",
    "df = pd.DataFrame()\n",
    "for file_ in all_files: \n",
    "    list_of_lists = []\n",
    "    with open(file_, 'rU') as f:\n",
    "        for line in f:\n",
    "            tweet = ' '.join(line.split(\")\")[1:])\n",
    "            id = line.split(\")\")[0].split(\",\")[0][1:]\n",
    "            sentiment = line.split(\")\")[0].split(\",\")[1]\n",
    "            try:\n",
    "                group = line.split(\")\")[0].split(\",\")[2]\n",
    "            except:\n",
    "                group = ''\n",
    "            list_of_lists.append([id, sentiment, group, tweet])\n",
    "    next_df = pd.DataFrame(list_of_lists, columns=['Id','Sentiment', 'Group', 'Tweet'])\n",
    "    df = df.append(next_df)\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"gôche\": \"gauche\",\n",
    "    \"gauchos\": \"gauche\",\n",
    "    \"réac\": \"réaction\",\n",
    "    \"co\": \"companie\",\n",
    "    \"cie\": \"companie\",\n",
    "    \"and\": \"et\",\n",
    "    \"but\": \"mais\",\n",
    "    \"ui\": \"oui\",\n",
    "    \"@macron\": \"Macron\",\n",
    "    \"@EmmanuelMacron\": \"Emmanuel Macron\",\n",
    "    \"GJ\": \"gilet jaune\",\n",
    "    \" 1 \": \" un \",\n",
    "    \"PR\": \"président\",\n",
    "    \"RT\": \"retweet\",\n",
    "    \"LREM\": \"la république en marche\",\n",
    "    \"LAREM\": \"la république en marche\",\n",
    "    \"#LREM\": \"la république en marche\",\n",
    "    \"#LAREM\": \"la république en marche\",\n",
    "    \"telma\": \"tellement\",\n",
    "    \"ctr\": \"contre\",\n",
    "    \"5e\": \"cinquieme\",\n",
    "    \"foute\": \"foutre\",\n",
    "    \"my god\": \"mon dieu\",\n",
    "    \"nn\": \"non\",\n",
    "    \"b8en\": \"bien\",\n",
    "    \"good\": \"bien\",\n",
    "    \"bad\": \"mauvais\",\n",
    "    \"lui\": \"il\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr')\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    decoded = str(text)\n",
    "    apostrophe_handled = re.sub(\"’\", \"'\", decoded)\n",
    "    expanded = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\n",
    "    parsed = nlp(expanded)\n",
    "    final_tokens = []\n",
    "    for t in parsed:\n",
    "        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\n",
    "            pass\n",
    "        else:\n",
    "            if t.lemma_ == '-PRON-':\n",
    "                final_tokens.append(str(t))\n",
    "            else:\n",
    "                sc_removed = re.sub(\"[^a-zA-Zéèêùûàâœçî]\", '', str(t.lemma_))\n",
    "                if len(sc_removed) > 1:\n",
    "                    final_tokens.append(sc_removed)\n",
    "    joined = ' '.join(final_tokens)\n",
    "    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_tweet'] = [tweet_cleaner(t) for t in df.Tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get stopwords from file\n",
    "with open('./stop_words_fr.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.Clean_tweet.values\n",
    "y = df.Sentiment.values\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(file):    \n",
    "    #Convert the file corpus to a dataframe\n",
    "    list_of_lists = []\n",
    "    with open(file, 'rU') as f:\n",
    "        for line in f:\n",
    "            tweet = ' '.join(line.split(\")\")[1:])\n",
    "            id = line.split(\")\")[0].split(\",\")[0][1:]\n",
    "            sentiment = line.split(\")\")[0].split(\",\")[1]\n",
    "            try:\n",
    "                group = line.split(\")\")[0].split(\",\")[2]\n",
    "            except:\n",
    "                group = ''\n",
    "            list_of_lists.append([id, sentiment, group, tweet])\n",
    "    df = pd.DataFrame(list_of_lists, columns=['Id','Sentiment', ' ', 'Tweet'])\n",
    "    print(df.info())\n",
    "    \n",
    "    #Clean and transform the data\n",
    "    df['Clean_tweet'] = [tweet_cleaner(t) for t in df.Tweet]\n",
    "    \n",
    "    X = df.Clean_tweet.values\n",
    "    y = df.Sentiment.values\n",
    "    \n",
    "    tv = TfidfVectorizer()\n",
    "    X = tv.fit_transform(X)\n",
    "    \n",
    "    #Use the saved classifier and evaluate the predictions\n",
    "    vec = open(\"rf_classifier.pickle\", 'rb')\n",
    "    clf = pickle.load(vec)\n",
    "    vec.close()\n",
    "    \n",
    "    #Use a container for the dataframe to make the prediction\n",
    "    container_df = pd.DataFrame(0.0, index=np.arange(len(X.todense())), columns=vectorizer.get_feature_names())\n",
    "    df_topredict = pd.DataFrame(X.todense(), columns=tv.get_feature_names())\n",
    "    for column in df_topredict:\n",
    "        if column in vectorizer.get_feature_names():\n",
    "            container_df[column] = df_topredict[column]\n",
    "\n",
    "    preds = clf.predict(container_df)\n",
    "    df['Predicted_sentiment'] = preds\n",
    "    print(df['Predicted_sentiment'])\n",
    "    \n",
    "    text_file = open(\"Annoted_Tweets.txt\", \"w\")\n",
    "    for index, row in df.iterrows():\n",
    "        line = '(' + row['Id'] + ', ' + row['Predicted_sentiment'] + ') ' + row['Tweet']\n",
    "        text_file.write(line)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: 'U' mode is deprecated\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      "Id           100 non-null object\n",
      "Sentiment    100 non-null object\n",
      "             100 non-null object\n",
      "Tweet        100 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.2+ KB\n",
      "None\n",
      "0     neg\n",
      "1     neg\n",
      "2     neg\n",
      "3     neg\n",
      "4     neu\n",
      "5     neg\n",
      "6     neg\n",
      "7     neg\n",
      "8     neg\n",
      "9     neg\n",
      "10    neg\n",
      "11    neg\n",
      "12    neg\n",
      "13    neg\n",
      "14    neg\n",
      "15    neg\n",
      "16    neg\n",
      "17    neg\n",
      "18    neg\n",
      "19    neg\n",
      "20    neg\n",
      "21    neu\n",
      "22    neg\n",
      "23    neg\n",
      "24    neg\n",
      "25    neg\n",
      "26    neg\n",
      "27    neg\n",
      "28    neg\n",
      "29    neg\n",
      "     ... \n",
      "70    neg\n",
      "71    neg\n",
      "72    neu\n",
      "73    neg\n",
      "74    neg\n",
      "75    neg\n",
      "76    neg\n",
      "77    neg\n",
      "78    neg\n",
      "79    neg\n",
      "80    irr\n",
      "81    neg\n",
      "82    neg\n",
      "83    neg\n",
      "84    neu\n",
      "85    neu\n",
      "86    neg\n",
      "87    neg\n",
      "88    neg\n",
      "89    neg\n",
      "90    neg\n",
      "91    neg\n",
      "92    neu\n",
      "93    neg\n",
      "94    neg\n",
      "95    irr\n",
      "96    neg\n",
      "97    irr\n",
      "98    neg\n",
      "99    neg\n",
      "Name: Predicted_sentiment, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "get_predictions('TweetsAboutMacron.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: 'U' mode is deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      "Id           100 non-null object\n",
      "Sentiment    100 non-null object\n",
      "             100 non-null object\n",
      "Tweet        100 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "list_of_lists = []\n",
    "with open('TweetsAboutMacron.txt', 'rU') as f:\n",
    "    for line in f:\n",
    "        tweet = ' '.join(line.split(\")\")[1:])\n",
    "        id = line.split(\")\")[0].split(\",\")[0][1:]\n",
    "        sentiment = line.split(\")\")[0].split(\",\")[1]\n",
    "        try:\n",
    "            group = line.split(\")\")[0].split(\",\")[2]\n",
    "        except:\n",
    "            group = ''\n",
    "        list_of_lists.append([id, sentiment, group, tweet])\n",
    "df = pd.DataFrame(list_of_lists, columns=['Id','Sentiment', ' ', 'Tweet'])\n",
    "print(df.info())\n",
    "\n",
    "#Clean and transform the data\n",
    "df['Clean_tweet'] = [tweet_cleaner(t) for t in df.Tweet]\n",
    "\n",
    "X = df.Clean_tweet.values\n",
    "y = df.Sentiment.values\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "X = tv.fit_transform(X)\n",
    "\n",
    "#Use the saved classifier and evaluate the predictions\n",
    "vec = open(\"rf_classifier.pickle\", 'rb')\n",
    "clf = pickle.load(vec)\n",
    "vec.close()\n",
    "\n",
    "#Use a container for the dataframe to make the prediction\n",
    "container_df = pd.DataFrame(0.0, index=np.arange(len(X.todense())), columns=vectorizer.get_feature_names())\n",
    "df_topredict = pd.DataFrame(X.todense(), columns=tv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_topredict:\n",
    "    if column in vectorizer.get_feature_names():\n",
    "        container_df[column] = df_topredict[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10332"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(container_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'neg', 'neg', 'neg', 'neu', 'neg', 'neg', 'neg', 'neg',\n",
       "       'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg',\n",
       "       'neg', 'neg', 'neg', 'neu', 'neg', 'neg', 'neg', 'neg', 'neg',\n",
       "       'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg',\n",
       "       'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg',\n",
       "       'neg', 'neg', 'neu', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg',\n",
       "       'neg', 'neg', 'neu', 'neg', 'neu', 'neg', 'pos', 'neg', 'neg',\n",
       "       'neg', 'pos', 'neg', 'neu', 'neg', 'neg', 'neg', 'neg', 'neg',\n",
       "       'neu', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'irr',\n",
       "       'neg', 'neg', 'neg', 'neu', 'neu', 'neg', 'neg', 'neg', 'neg',\n",
       "       'neg', 'neg', 'neu', 'neg', 'neg', 'irr', 'neg', 'irr', 'neg',\n",
       "       'neg'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(container_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
