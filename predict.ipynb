{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import pickle\n",
    "import codecs\n",
    "import spacy\n",
    "import glob\n",
    "import re\n",
    "\n",
    "all_files = glob.glob(\"/corpus/groupe*.txt\")\n",
    "df = pd.DataFrame()\n",
    "for file_ in all_files: \n",
    "    list_of_lists = []\n",
    "    with open(file_, 'rU') as f:\n",
    "        for line in f:\n",
    "            tweet = ' '.join(line.split(\")\")[1:])\n",
    "            id = line.split(\")\")[0].split(\",\")[0][1:]\n",
    "            sentiment = line.split(\")\")[0].split(\",\")[1]\n",
    "            try:\n",
    "                group = line.split(\")\")[0].split(\",\")[2]\n",
    "            except:\n",
    "                group = ''\n",
    "            list_of_lists.append([id, sentiment, group, tweet])\n",
    "    next_df = pd.DataFrame(list_of_lists, columns=['Id','Sentiment', 'Group', 'Tweet'])\n",
    "    df = df.append(next_df)\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"gôche\": \"gauche\",\n",
    "    \"gauchos\": \"gauche\",\n",
    "    \"réac\": \"réaction\",\n",
    "    \"co\": \"companie\",\n",
    "    \"cie\": \"companie\",\n",
    "    \"and\": \"et\",\n",
    "    \"but\": \"mais\",\n",
    "    \"ui\": \"oui\",\n",
    "    \"@macron\": \"Macron\",\n",
    "    \"@EmmanuelMacron\": \"Emmanuel Macron\",\n",
    "    \"GJ\": \"gilet jaune\",\n",
    "    \" 1 \": \" un \",\n",
    "    \"PR\": \"président\",\n",
    "    \"RT\": \"retweet\",\n",
    "    \"LREM\": \"la republique en marche\",\n",
    "    \"nn\": \"non\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr')\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    decoded = str(text)\n",
    "    apostrophe_handled = re.sub(\"’\", \"'\", decoded)\n",
    "    expanded = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\n",
    "    parsed = nlp(expanded)\n",
    "    final_tokens = []\n",
    "    for t in parsed:\n",
    "        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\n",
    "            pass\n",
    "        else:\n",
    "            if t.lemma_ == '-PRON-':\n",
    "                final_tokens.append(str(t))\n",
    "            else:\n",
    "                sc_removed = re.sub(\"[^a-zA-Zéèêùûàâœçî]\", '', str(t.lemma_))\n",
    "                if len(sc_removed) > 1:\n",
    "                    final_tokens.append(sc_removed)\n",
    "    joined = ' '.join(final_tokens)\n",
    "    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_tweet'] = [tweet_cleaner(t) for t in df.Tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get stopwords from file\n",
    "with open('./stop_words_fr.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.Clean_tweet.values\n",
    "y = df.Sentiment.values\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(file):    \n",
    "    #Convert the file corpus to a dataframe\n",
    "    list_of_lists = []\n",
    "    with open(file, 'rU') as f:\n",
    "        for line in f:\n",
    "            tweet = ' '.join(line.split(\")\")[1:])\n",
    "            id = line.split(\")\")[0].split(\",\")[0][1:]\n",
    "            sentiment = line.split(\")\")[0].split(\",\")[1]\n",
    "            try:\n",
    "                group = line.split(\")\")[0].split(\",\")[2]\n",
    "            except:\n",
    "                group = ''\n",
    "            list_of_lists.append([id, sentiment, group, tweet])\n",
    "    df = pd.DataFrame(list_of_lists, columns=['Id','Sentiment', ' ', 'Tweet'])\n",
    "    print(df.info())\n",
    "    \n",
    "    #Clean and transform the data\n",
    "    df['Clean_tweet'] = [tweet_cleaner(t) for t in df.Tweet]\n",
    "    \n",
    "    X = df.Clean_tweet.values\n",
    "    y = df.Sentiment.values\n",
    "    \n",
    "    tv = TfidfVectorizer()\n",
    "    X = tv.fit_transform(X)\n",
    "    \n",
    "    #Use the saved classifier and evaluate the predictions\n",
    "    vec = open(\"rf_classifier.pickle\", 'rb')\n",
    "    clf = pickle.load(vec)\n",
    "    vec.close()\n",
    "    \n",
    "    #Use a container for the dataframe to make the prediction\n",
    "    container_df = pd.DataFrame(0.0, index=np.arange(len(X.todense())), columns=vectorizer.get_feature_names())\n",
    "    df_topredict = pd.DataFrame(X.todense(), columns=tv.get_feature_names())\n",
    "    for column in df_topredict:\n",
    "        if column in vectorizer.get_feature_names():\n",
    "            container_df[column] = df_topredict[column]\n",
    "\n",
    "    preds = clf.predict(container_df)\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: 'U' mode is deprecated\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      "Id           100 non-null object\n",
      "Sentiment    100 non-null object\n",
      "             100 non-null object\n",
      "Tweet        100 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.2+ KB\n",
      "None\n",
      "['neg' 'neg' 'neg' 'neg' 'neu' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg'\n",
      " 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neu' 'neg' 'neg'\n",
      " 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'pos' 'neg' 'neg' 'neg' 'neg'\n",
      " 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neu'\n",
      " 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neu' 'neg' 'neu' 'neg'\n",
      " 'pos' 'neg' 'neg' 'neg' 'pos' 'neg' 'neu' 'neg' 'neg' 'neg' 'neg' 'neg'\n",
      " 'neu' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'irr' 'neg' 'neg' 'neg'\n",
      " 'neu' 'neu' 'neg' 'neg' 'neg' 'neg' 'neg' 'neg' 'neu' 'neg' 'neg' 'irr'\n",
      " 'neg' 'irr' 'neg' 'neg']\n"
     ]
    }
   ],
   "source": [
    "get_predictions('TweetsAboutMacron.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
